
1) Aprendizado Semi-Supervisionado
    Link ref: 
        https://lamfo-unb.github.io/2017/05/09/Aprendizado-Semi-Supervisionado-para-Deteccao-de-Fraudes-Parte-1/
        https://lamfo-unb.github.io/2017/05/11/Aprendizado-Semi-Supervisionado-para-Deteccao-de-Fraudes-Parte-2/
        https://medium.com/@edmauryaishu/factor-analysis-researching-hidden-variables-pca-798783def458
        https://lamfo-unb.github.io/2017/05/12/Aprendizado-Semi-Supervisionado-para-Deteccao-de-Fraudes-Parte-3/
    Fichamento:
        Principal Component Analysis(PCA) is a great to reduce dimensionality and get rid of high correlation among independent variables. However, PCA abstracts away original meaning of variables and makes it harder to attach business logic to the variables being analysed.
        anomalias podem ser o resultado de atividade maliciosa, nesse caso, o adversário está sempre tentado se adaptar para fazer com que as observações anômalas pareçam noramis.
        Aprendizado de máquina é uma ciência que utiliza métodos da ciência da computação e estatística para analisar dados.
        As técnicas de aprendizado de máquina surgiram dentro do campo de inteligência artificial, como um meio de permitir que os computadores aprendessem uma forma de conhecimento própria.
        Dentro dos regimes de aprendizado de máquina, destaca-se o de aprendizado de máquina supervisionado, que foca em problemas de previsão: tendo uma base de dados com “alvos” para cada observação (pares (x,y)), a meta é aprender quais “alvos” (y) estão associados a quais dados (x).
        Alguns exemplos desse tipo de problema são: identificar a presença de uma doença (alvo), dado os sintomas do paciente (dados); prever se o preço da ação de uma empresa vai subir ou cair (alvo), dado o histórico do mercado financeiro (dados); identificar de que pessoa é a face (alvo) em uma imagem (dados) ou classificar um livro (dados) em uma escola literária (alvo).        
        Em cenários de detecção de anomalias, muitas vezes nós só temos dados que representam o caso normal, sendo extremamente difícil conseguir dados que representam anomalias.
        Uma possibilidade é utilizar aprendizado de máquina semi-supervisionado, em que consideramos apenas uma pequena parcela dos dados como estando nomeada e que os dados não nomeados só contêm exemplos do caso normal, que geralmente é abundante.
        Assim, nós utilizamos técnicas de aprendizado não supervisionado (para aprender a estrutura dos dados) nos dados não nomeados para extrair alguma noção de normalidade.
        Ao final dessa etapa não supervisionada, a máquina conseguirá associar cada observação com uma pontuação proporcional à probabilidade do dado ter vindo da região normal. 
        Em problemas de previsão, a métrica de avaliação mais comum é a acurácia, que mede a proporção de acertos.
        No entanto, nesse cenário, como mais de 99% dos dados pertencem a uma única categoria, um preditor ingênuo prevendo todas as observações como sendo normais já conseguiria mais de 99% de acerto.
        Isso nos motiva a utilizar outras métricas: precisão e revocação.
        Assim, um sistema ideal teria que ponderar precisão e revocação em algum ponto ótimo.
        F2-score, que combina revocação e precisão, dando mais importância a primeira.

        Técnicas de Detecção de Anomalias 
            Modelo Gaussiano
                Intuitivamente, se um fenômeno tem comportamento gaussiano, podemos dizer que 95% das realizações desse fenômeno aconteceram a no máximo 2 desvios padrões de distância da média.
                Por exemplo, se a quantidade média transacionada por cartão de crédito for de R$ 50,00, com um desvio padrão de R$ 10,00, podemos dizer que 95% das transações serão de um valor entre 30 e 70 reais.
                Quando encontramos a gaussiana multidimensional que melhor se encaixa nos dados, nós podemos extrair a probabilidade de cada observação, que seria a altura da gaussiana multidimensional.
            Modelo Histograma
                Nem todas as variáveis dos nossos dados seguem uma distribuição gaussiana.
                A começar pela distribuição da variável que mede a quantidade transacionada, que tem uma distribuição bastante complicada, tanto para os dados normais quanto para os anormais. 
                vscode://file/D:/IFB/ptcc/image/hist1.png
                vscode://file/D:/IFB/ptcc/image/hist3.png
                Quando vemos esses histogramas, notamos facilmente como algumas variáveis são ótimos indicadores de anomalias, uma vez que a distribuição (aproximada pelo histograma) dos dados anômalos e dos dados normais são bastante distinguíveis. Veja por exemplo a variável V10.
            Modelo de Mistura de Gaussianas
                Como vimos nos histogramas acima, nem todas as variáveis seguem um modelo gaussiano. Nós podemos utilizar um modelo que relaxa essa hipótese, assumindo que os dados vêm de uma distribuição que é uma mistura de gaussianas.
                De fato, contanto que tenhamos gaussianas suficiente, qualquer distribuição poderá ser aproximada por uma mistura de gaussiana.
            Modelo de Máquina de Suporte Vetorial
                Máquinas de Suporte Vetoriais (MSV) são uma classe de modelos originalmente desenhadas para problemas supervisionados de categorização (ou classificação), em que o objetivo é separar dois tipos de observação.
                Intuitivamente, o que os MSVs fazem é achar o melhor plano de separação entre os dois tipos de dados.
                Para superar essa limitação, as MSV fazer uso do truque do kernel, que representa os dados originais em um espaço dimensional maior, de forma que seja possível separar os dois tipos nesse novo espaço.
                vscode://file/D:/IFB/ptcc/image/kerneltrick.png
                Na versão não supervisionada da MSV, não estamos interessados em separar dois tipos de dados, pois os dados sequer são nomeados com os tipos. Em vez disso, queremos achar a melhor esfera que encapsule todos os dados. 
                vscode://file/D:/IFB/ptcc/image/oneclasssvm.png
                Na verdade, nós podemos dizer que o modelo não supervisionado de MSV é um aproximador universal de funções densidade de probabilidade, isto é, ele é capaz de aproximar qualquer distribuição possível.
                Uma severa desvantagem desse método é o fato dele ser extremamente ineficiente para treinar. 
            Modelo de Floresta de Isolação
                O modelo de floresta de isolação é outro modelo que podemos considerar como um aproximador universal de distribuições.
                O modelo de floresta de isolação ajusta várias árvores de isolação aos dados.
                Para construir cada árvore de isolação, primeiro selecionamos aleatoriamente uma das variáveis nos dados.
                Em seguida, selecionamos um valor aleatório entre o máximo e o mínimo dessa variável, que será utilizado para separar os dados.
                Nós continuamos fazendo essas segmentações aleatórias até que todas as observações estejam isoladas, isto é, separadas das demais. 
                Nossa esperança é que as anomalias sejam distintas dos dados normais, sendo que esses se aglomeram em algum local do espaço, enquanto que aquelas ficam mais isoladas.                
                Cabe ainda ressaltar que vantagens dessa metodologia frente às máquinas de suporte vetorial são a velocidade de treinamento e a capacidade de fácil paralelização.
            Modelo Neural
                Nos últimos anos, redes neurais se tornaram o canivete suíço de aprendizado de máquina, tanto pela sua flexibilidade e efetividade na maioria dos cenários que envolvem problemas estatísticos de alta complexidade e não linearidade. 
                Para impedir que a rede neural simplesmente copie o que lhe foi passado como reconstrução perfeita, nós colocamos uma camada de neurônios estreita no meio da rede neural.
                Essa cama terá apenas dois neurônios e como os nossos dados têm 30 variáveis, isso significa que a rede neural terá que aprender uma representação interna que condense 30 dimensões em apenas duas.
        
        Métodos semi-supervisionados de aprendizado de máquina são excelentes em cenários de detecção de anomalias, em que há pouquíssimos exemplos do caso anômalo. 



    Anotações pessoais.
        alvo é situação almejada que a máquina identifique.
        alvo positivo é a situação desejada.
        alvo negativo é a situação não desejada?
        técnica PCA permite manter a informação integral dos dados ao mesmo tempo que retira sua interpretabilidade. 
        existe um tradeoff entre precisão e revocação.
        revocação = recall = revocabilidade = o quão completos os resultados estão = medida de completude ou quantidade
        precisão = o quanto os resultados da pesquisa são úteis = medida de exatidão ou qualidade
        
2) PMML - PREDICTIVE MODEL MARKUP LANGUAGE
    LINK REF:
        https://repositorio.unb.br/bitstream/10482/16954/1/2014_JoseAbiliodePaivaRamos.pdf
        https://www.ibm.com/developerworks/library/ba-ind-PMML1/index.html
    Fichamento:
        Uma vez que o sucesso de uma hipótese ou modelo induzido por um algoritmo de AMdepende dos dados disponíveis [40], são utilizadas técnicas para a melhoria da qualidadedos mesmos.
        PREPARAÇÃO DOS DADOS
            ELIMINAÇÃO MANUAL DE ATRIBUTOS
                Outros atributos que não agregaminformação útil são números sequenciais de controle, como identificadores de registro.
            INTEGRAÇÃO DE DADOS
            AMOSTRAGEM DE DADOS
                Algoritmos de AM podem ter dificuldades no trato de amostra de dados com muitosobjetos, em virtude das respectivas complexidades computacionais.  
                Para resolver esteproblema, usa-se um subconjunto dos dados.
            BALANCEAMENTO DE DADOS
                Em vários conjuntos de dados reais, o número de objetos varia para as diferentes classese isto pode constituir um problema para vários algoritmos de AM, pois estes algoritmospodem ter dificuldades para aprender o conceito relacionado à classe minoritária.
                Quando alimentados com dados desbalanceados, esses algoritmos tendem a gerar ummodelo que favorece a classificação de novos dados na classe majoritária
            LIMPEZA DE DADOS
                Os dados com ruídos podem levar a um superajuste do modelo, pois o algoritmoque induz o modelo pode se ater às especificidades relacionadas aos ruídos, em vez dadistribuição verdadeira que gerou os dados; por outro lado, a eliminação desses dadospode fazer com que algumas regiões do espaço de atributos não sejam consideradas noprocesso de indução de hipóteses.
            REDUÇÃO DA DIMENSIONALIDADE
                Para que objetos com um número elevado de atributos possam ser utilizados em mui-tos algoritmos de AM, a quantidade de atributos precisa ser reduzida.
                As técnicas de redução de dimensionalidadepodem ser divididas em duas grandes abordagens: agregação e seleção de atributos.
                Enquanto as técnicas de agregação substituem parte dos atributos originais por novos atributos formados pela combinação de grupos de atributos, as técnicas de seleção mantêmuma parte dos atributos originais e desconsideram os demais.
                A Análise de Componentes Principais ou Principal Component Analysis (PCA)é um exemplo de técnica de agregação.
                    Este método busca combinações lineares, chamada sde componentes principais, que capturam, de maneira resumida, a maior variabilidade possível dos dados.
                    A principal vantagem da PCA como método de redução de dimensionalidade encontrase na construção de componentes não-correlacionados.
                    O uso de atributos correlacionadosem alguns algoritmos de AM, como regressão logística ou redes neurais, pode introduzir erro e diminuir o desempenho do algoritmo.
                    Uma vez que não há relacionamento entre os atributos e os rótulos dos objetos, os componentes principais não provêem um relacionamento apropriado com as categorias dos objetos em tarefas de aprendizado supervisionado.
                Testeχ2
                    Por sua vez, as técnicas de seleção avaliam a relevância do atributo antes da aplicaçãode algum modelo de AM. 
                    Somente os atributos correlacionados com os rótulos dos objetos serão considerados na aplicação do modelo.
                    O grau de associação entre duas variáveis analisadas pelo testeχ2pode ser representado pelocoeficiente de contingência de Pearson (CC).
                    O coeficiente de contingência é nulo quando não há associação entre as variáveis equanto maior o valor de CC, maior será a associação entre as variáveis.
            TRANSFORMAÇÃO DE DADOS
                As técnicas de transformação de dados podem ser utilizadas para converter valores simbólicos em valores numéricos, ou vice-versa.
                [POIS] Alguns algoritmos de AM estão limitados à manipulação de valores de determinados tipos, por exemplo, apenas valores numéricos (técnicas como redes neurais e máquinas de vetores de suporte) ou apenas valores simbólicos (árvores de decisão ID3, por exemplo).
                Conversão Simbólico-Numérico
                    Para um atributo simbólico com mais de dois valores, a técnica utilizada na conversão depende de o atributo ser nominal ou ordinal.
                    Se não houver relação de ordem entre os atributos simbólicos, esta propriedade deve ser mantida para os valores numéricos gerados.
                    distância de Hamming: a distância entre duas sequências binárias com mesmo númerode elementos é igual ao número de posições em que as sequências apresentam valores diferentes.
                    Se existe uma relação de ordem entre os valores nominais, a codificação deve manter essa propriedade:  basta ordenar os valores categóricos ordinais e codificar cada valor de acordo com sua posição na ordem, utilizando-se números inteiros ou reais.
                Conversão Numérico-Simbólico
                    Se o atributo original for formado por sequências binárias sem uma relação de ordem entre si, cada sequência pode ser substituída por um nome ou categoria.
                Transformação de Atributos Numéricos
                    Algumas vezes, o valor numérico de um atributo precisa ser transformado em outro valor numérico; 
                    Uma transformação que é muito utilizada é a normalização de dados: a cada valor do atributo a ser normalizado é adicionada (ou subtraída) uma medida de localização e o valor resultante é em seguida multiplicado (ou dividido) por uma medida de escala.
        MODELOS PREDITIVOS
            Um algoritmo de AM preditivo produz, dado um conjunto de objetos rotulados, um modelo capaz de produzir previsões a respeito dos objeto.
            Os rótulos dos objetos assumem valores em um domínio conhecido.
            Se o domínio for um conjunto infinito e ordenado de valores, tem-se um problema de regressão e o modelo gerado é um regressor; por outro lado, se esse domínio for um conjunto de valores nominais, tem-se um problema de classificação e o modelo gerado é um classificador.
            Os classificadores podem ser gerados por diversos algoritmos de AM preditivos, por exemplo: k-NN, naive Bayes, máquinas de vetores de suporte, redes neurais artificiais (RNA), árvores de decisão. 
            Por aderência ao problema deste trabalho, foramescolhidas árvores de decisão para a identificação de transações fraudulentas.
        TRABALHOS CORRELATOS
            A maioria dos trabalhos publicados sobre detecção de fraudes está relacionada ao do-mínio de cartão de crédito, concessão de crédito, intrusão de computadores e fraudes em serviços de telecomunicações.
            Kovach propõe uma arquitetura de um sistema de detecção de fraudes bancárias, realizadas através daInternet, em tempo real baseada na teoria matemática de evidências de Dempster-Shafer. [NAO É POSSÍVEL COMPARAR COM A ATUAL ESTUDO]
        ÁRVORES DE DECISÃO
            Uma árvore de decisão usa a estratégia dividir para conquistar na solução de um problema de decisão: um problema complexo é dividido em problemas mais simples, aos quais recursivamente é aplicada a mesma estratégia; as soluções dos subproblemas podem ser combinadas, na forma de uma árvore, para gerar uma solução do problema complexo.
            Formalmente, uma árvore de decisão é um grafo acíclico direcionado em que cada nó ou é um nó de divisão, com dois ou mais sucessores, ou um nó folha.
            INDUÇÃO DE ÁRVORES DE DECISÃO
                O processo de construção de uma árvore a partir de uma amostra de treinamento é conhecido como indução da árvore.
            [...]                
        MEDIDAS DE DESEMPENHO
        PMML
            utilizado para representar modelos de mineração de dados e permite a representação de diversos modelos como árvores de decisão, redes neurais ou classificador naive Bayes.
            A estrutura geral de um documento PMML é composta por um cabeçalho, dicionário de dados, transformações de dados e modelo.
        MATERIAL E MÉTODO
            Cada objeto nesta base de dados possuem dois atributos com função de rótulo: um atributo para identificar as transações que foram apontadas como fraudulentas pelas regras ad-hoc elaboradas pelos especialistas da instituição fornecedora dos dados e outro para indicar quais estão rotuladas como fraudulentas.
            METODO
                [NESTE] trabalho será utilizada a metodologia de mineração de dados CRISP-DM (CrossIndustry Standard Process for Data Mining) para condução do projeto de construção do classificador para identificação de transferências bancárias fraudulentas, pois é uma metodologia madura para guiar projetos de mineração de dados.
                O ciclo de vida do projeto de mineração, nesta metodologia, é composto de seis fases: entendimento do negócio, entendimento dos dados, preparação dos dados, modelagem, avaliação e implantação.
        EXPERIMENTOS
            ENTENDIMENTO DO NEGÓCIO
                o custo financeiro de uma transação fraudulenta não alertada, falso-negativo, é superior ao custo de uma transação legítima apontada como fraudulenta, falso-positivo; pois quando uma transação legítima é apontada como fraudulenta, o tratamento deste alerta envolve o custo da mão-de-obra necessária para analisá-lo.
                Por suavez, se a fraude não for detectada, além do prejuízo financeiro, a conta vitimada pode ser utilizada para recebimento de outros créditos fraudulentos.
                Neste contexto, a identificação de transações fraudulentas baseada em métodos ad-hoc,nos quais as regras de detecção são exclusivamente escritas por especialistas do domínio,não é aceitável, pois, normalmente são muito laboriosas e requerem muito tempo para elaboração e implantação ficando desatualizadas rapidamente.
                Cada objeto nesta base de dados é composto por 10 atributos de tipo numérico discreto, 7 atributos de tipo numérico contínuo e 5 atributos categóricos.
                Para minimizar o problema das classes extremamente desbalanceadas foram aplicadas duas regras definidas pelos especialistas do domínio que excluem transações que rara-mente são fraudulentas, estas regras especialistas baseiam-se em perfis transacionais e naagregação de atributos para a extração de informação latente nos dados.
            PRPARAÇÃO DOS DADOS
                Estes atributos foram nomeados como v01,v02, ...,v041,v_regra,alvo; onde v_regra indica os alertas das regras ad-hoc e alvo corresponde ao rótulo das transações.
                foi utilizado o teste χ2 para estimar o grau de associação entre os atributos nominais/numéricos discretos e o rótulo fraude o unão-fraude, representado pela variável alvo.
                Com relação à representatividade das amostras, geralmente não é possível afirmar se uma amostra é representativa ou não, para mitigar este problema é utilizado o processo de estratificação: cada classe no conjunto de dados original deve ser representada na mesma proporção nas amostras de teste, validação e treinamento.
            MODELAGEM
                Para indução do classificador utilizou-se o algoritmo C5.0 um sistema inicialmente comercial da Rule Quest Research e agora também distribuído sob a licença GNU GPL.
            ARVORES DE DECISÃO
                O algoritmo 5.0 constrói a árvore através a divisão da amostra de treinamento com base no teste que resulta na maior razão de ganho.
                Cada subconjunto obtido da primeira divisão é novamente divido pela aplicação de um novo teste e este processo é repetido até que nenhuma outra divisão seja possível.
                Por fim, a simplificação da árvore com a poda dos nós que não contribuem para a tarefa de classificação é realizada através da poda pessimista embutida no C5.0.
                BALANCEAMENTO
                    Muitos algoritmos de classificação têm sua acurácia prejudicada quando as classes possuem quantidades de objetos muito diferentes.
                    Nesta situação a classe majoritária pode ser reduzida, a classe minoritária pode ser inflada ou uma combinação de ambas as técnicas.
                PODA
                    A extensão da poda da árvore é determinada pelo parâmetro nível de confiança que possui valor padrão de 25%.
                    A diminuição deste valor resulta em árvores menores e mais concisas, privilegiando a capacidade de generalização do modelo; por outro lado, o aumento do nível de confiança é usado para a obtenção de árvores de maior acurácia, devido ao maior ajustamento aos dados de treinamento.
                CRITERIO DA PARADA
                    A árvore gerada pelo algoritmo C5.0 cresce até que todos os objetos em um nó folha pertençam à mesma classe ou o número de objetos nos nós resultantes da aplicação de um dado teste condicional não sejam inferiores a um dado limiar.
                APLICAÇÃO DE CUSTOS
                    O algoritmo C5.0 permite a utilização de custos, quando da ocorrência de classificações incorretas, visando ao aumento da acurácia da árvore de decisão gerada.
            REDES NEURAIS ARTIFICIAIS
                Para comparação com o modelo gerado através do algoritmo C5.0, foram construídas redes neurais artificais perceptron multicamadas, pois este tipo de rede permite a representação de relacionamentos complexos entre os dados.
            IMPLANTAÇÃO
                Nesta etapa foi realizada a incorporação do classificador obtido, a árvore de decisão, no processo de negócio.
                A árvore de decisão no formato PMML foi interpretada por uma aplicação construída com base na API Java JPMML-Evaluator, Java Evaluator API for Predictive Model Markup Language (PMML), após a inclusão de classes para comunicação com os bancos de dados utilizados.
                A atualização é realizada sob demanda quando do surgimento de uma nova modalidade de ataque ou quando observa-se redução nas métricas precisão e sensibilidade.
                A cada atualização é gerada uma nova árvore de decisão que é exportada no formato PMML para utilização na camada de avaliação de transações.
                A integração destas duas camadas permitiu imprimir celeridade na atualização dos modelos de identificação de fraudes, pois cada nova árvore é gerada, em média, em 15 minutos.
                
3) 
    LINK REF: http://bibliotecadigital.fgv.br/dspace/bitstream/handle/10438/27166/Dissertacao_Joao_Carlos_Pacheco_VFinal_2.pdf?sequence=3&isAllowed=y
    FICHAMENTO:
        O ojetivo do trabalho é identificar transações de crédito fraudulentas após a primeira aprovação de crédito.
        REVISAO TEORICA - Técnicas de amostragem de dados e medidas de avalição dos modelos preditivos:
            regressão logística
                funcao resposta
                estimação dos parâmetros
            aprendizado de máquina  
                aprendizado supervisionado
                    No  aprendizado  supervisionado  como  apresentado  por  Barber  (2012)  existe uma base de dados prévios com as características dos dados que denotaremos por 𝑥 e  sua  classificação  que  denotaremos  por 𝑦.
                    Neste  processo  é  criada  uma  base de treinamento, com uma amostra dos dados, onde o algoritmo irá identificar quais características presentes em 𝑥 são suficientes para chegar à classificação de 𝑦.
                aprendizado não supervisionado
                    No aprendizado não-supervisionado não existe uma base de dados prévia que permite que os modelos sejam desenvolvidos com uma base de treinamento, portanto o  modelo  deverá  ser  executado  conforme  as  informações  são  geradas.
                    Esta  abordagem visa identificar padrões em bases de dados, mas sem a existência de uma resposta prévia para a classificação que será identificada.
                aprendizado semi-supervisionado
                    No aprendizado de máquina, é comum possuir uma pequena quantidade de dados rotulados frente a uma grande quantidade de dados não rotulados.
                    No aprendizado semi-supervisionado, utiliza-se os dados não rotulados para tentar  criar  um  classificador  melhor  do  que  o  criado  com  base  apenas  nos  dados rotulados.
                maquina de vetores de suporte
                    possui grande capacidade na detecção de padrões, sendo assim capazes de identificar operações fraudulentas de cartões de créditos, identificar dígitos escritos a mão, além de serem utilizados coms ucesso em várias aplicações biológicas
                rede bayesianas
                    Ao analisarmos um problema, muitas vezes podemos acreditar que um evento afeta outro e existem eventos que são independentes. 
                    para que este tipo de informação seja utilizado de forma eficiente utilizamos estruturas de grafos que nos permitem descrever como os objetos são vinculados e fornecem uma imagem conveniente da relação dos objetos (eventos).
                metodos de emsemble
                    Os métodos Ensemble possuem como principal objetivo combinar o poder preditivo de vários estimadores por meio de um algoritmo de aprendizado, visando aperfeiçoar a generalização e robustez quando comparado a um estimador utilizado sozinho.
                bootstrap aggregating - baggind
                    Este procedimento possui a característica de melhorar o desempenho de pre-ditores instáveis que são basicamente os preditores com alta variância
                boosting
                    o método deBoostingse refere a família de algoritmos que consegue converter preditores fracos em fortes preditores.
                bases de dados desbalanceadas
                    Uma das características marcantes das bases de dados que possuem infor-mações de fraude é o desbalanceamento.
                    os  principais  métodos  utilizados  para  modelagem  em  base  de  dados desbalanceadas, sendo que neste trabalho foi utilizada a técnica deRandom Under-sampling.
                    O  procedimento  de oversampling aleatório  tem  como  mecanismo  criar  uma amostra da classe minoritária maior que a disponível, ou seja, é criado um conjuntode exemplos selecionados de forma aleatória da classe minoritária 𝑆𝑚𝑖𝑛, que são inseridos na amostra 𝐸, posteriormente esta subamostragem é inserida na base de testesoriginal 𝑆.
                selecao de variaveis
                    Portanto foi selecionado para os modelos lineares a técnica de Stepwise, já para os modelos que utilizam métodos de aprendizado de máquina utilizamos o procedimento de seleção sequencial usando o modelo de Random Forest.
                métricas de avaliação de desempenho do modelo   
                    A representação da performance da classificação também pode ser formulada por meio da matriz de confusão que descreve os custos de se fazer atribuição das classes às amostras, portanto cada uma das linhas da matriz contém cada classe passível de ser atribuída às instâncias pelo classificador, enquanto cada coluna possui a classe à qual as amostras efetivamente pertencem na realidade.
                    A métrica de Precision fornece a informação de precisão. 
                        Portanto podemos dizer que está métrica é intuitivamente a capacidade do classificador não rotular como positiva uma amostra que é negativa.
                    A métrica de Recall fornece informação de recordação.
                        Portanto podemos dizer que está métrica é intuitivamente a capacidade do classificado encontrar todas as amostras positivas.
                    métrica de F-Mensure, que também é conhecida como F-score ou F1 score,é a combinação das métricas anteriores de recordação e precisão e pode ser interpretada como uma média ponderada destas métricas, assim a métrica F-Mersure alcançao seu melhor valor em 1 e o pior em 0.
                    receiver operating characterisctics (ROC) curves
                    teste kolmogorov-smirnov
        METODOLOGIA
            etapas: pre processamento, construção de amostras, seleção das variaveis e aplicacao dos modelos
            utilizou python para todos exceto na avaliação de desempenho que utilizou o R
            A base de treinamento foi a utilizada para treinar o classificador, visando construir um modelo, por sua vez a base de teste é utilizada para identificar o poder de generalização do modelo, sendo que esta base não foi utilizada no desenvolvimento do modelo.
            foi realizada uma amostragem da base de treinamento, visando deixar o índice de desbalanceamento baixo, tornando a proporção de casos positivos iguais aos casos negativos.
            utilizamos o procedimento de undersample que seleciona todos os casos  positivos,  logo  em  seguida  seleciona  de  forma  randômica  na  base  de  teste a mesma quantidade de casos negativos, tornando assim a base balanceada.
            Assim aplicamos um método que identifica colinearidade acima de 98% entre as variáveis presentes na base de dados, de tal modo que dos pares de recursos com alta colinearidade apenas um é selecionado para remoção, sendo que apenas uma das variáveis precisa ser removida.
            O procedimento adotado necessitadas variáveis que iremos prever neste trabalho, buscando identificar quais variáveis possuem importância zero utilizando o modelo de aprendizado Gradient Boosting.
            visa identifica a existência de alguma variável que possua um valor único, pois este tipo de variável possui o comportamento semelhante ao de uma constante.
            nesta etapa foi realizada uma avaliação que garanta que os modelos não incorram no erro de utilizar poucas variáveis, gerando assim um underfitting
            Para os modelos lineares utilizamos a técnica de Stepwise, para selecionar asvariáveis para o modelo.
            foi utilizada a linguagem de programaçãoPythoncom o projeto scikit-learn.
            NumPy,SciPy,Matplotlib e Pandas.
            O segundo passo do trabalho foi criar um modelo capaz de predizer todas as variáveis respostas ao mesmo tempo. De tal modo como técnica selecionada neste trabalho, foi utilizado o algoritmo Randon Forest que possui suporte para classificação multi-output, ou seja, com múltiplas saídas.
            Para realizar um contraponto no método de multi-output nativo do classificador Ensenble Randon Forest foi utilizado o procedimento implantado pelo projeto scikit-learn MultiOutputClassifier, que fornece um meio de utilização de qualquer classificador para uma classificação de múltiplas variáveis respostas
            Ao avaliar todos os resultados a resposta não pode ser direta pois existe um “perde-e-ganha”  quando  avaliamos  todos  os  resultados  conjuntamente,  sendo  que cada uma das metodologias possui as suas vantagens e desvantagens.
            Nesta etapa do trabalho, serão avaliadas as variáveis selecionadas pelo método de seleção sequencial, com uso do classificador RF, para os modelos que utilizam técnicas de aprendizado de máquina.
            A única variável que esteve presente no treinamento de todos os modelos, independente da variável resposta foi a que identifica a quantidade de responsáveis pelos domicílio particular.

4)
    LINK  REF: https://medium.com/ensina-ai/detectando-fraudes-financeiras-usando-aprendizado-de-m%C3%A1quina-ganhando-a-guerra-contra-dados-3280893d09cb
    FICHAMENTO:
        No Aprendizado de Máquina, problemas como a detecção de fraudes são geralmente considerados problemas de classificação — prever a classe correspondente a uma dada observação.
        o problema de classificação requer a criação de modelos com inteligência suficiente para classificar adequadamente as transações como sendo legítima ou fraudulenta, a partir de detalhes tais como valor, estabelecimento, localização, data e hora, entre outros.
        O principal desafio que enfrentamos, ao modelar a detecção de fraudes como um problema de classificação, reside no fato que a grande maioria das transações não é fraudulenta. 
        E uma das principais missões de um Cientista de Dados é criar valor para o negócio a partir dos dados.
        Esses dados correspondem a transações ocorridas em dois dias, onde observa-se 492 fraudes dentre 284.807 transações. 
        Esses dados são altamente desbalanceados, sendo que a classe positiva (fraudes) representa apenas 0,172% de todas as transações.
        Sempre é bom fazer uma EDA — Análise Exploratória de Dados antes de começar a trabalhar em modelos de previsão e análise. 
        maneiras de tratar dados desbalanceados:
            oversampling (super amostragem) - smote (synthetic minotiry over-sampling techinique)
                realizar uma super-amostragem significa criar artificialmente novas observações em nosso conjunto de dados que pertençam à classe que está sub-representada.
                realiza os seguintes passos:
                    1) busca por k-nearest-neighbors (observações similares) da classe minoritária
                    2) coleta um dos k-nearest-neighbors e o utiliza para criar novas obserções artificialmente.
                existem várias implementações smote, tais como: imblearn
                REF: https://jair.org/index.php/jair/issue/view/1100
            undersampling (sub amostragem) - random under sampler
                Na sub-amostragem, é a classe dominante que sofre amostragem, a fim de reduzir o seu número de observações.
                Isso pode ser feito com a escolha aleatória de um conjunto reduzido de amostras.
                A classe RandomUnderSampler da biblioteca imblearn é uma forma fácil e rápida de balancear os dados através da seleção aleatória de um subconjunto de dados das classes escolhidas.  
            métodos de classes combinadas - smote + enn
                A técnica SMOTE pode acabar gerando novas amostras indesejadas quando, ao fazer a interpolação entre dois pontos, um deles é marginalmente outlier.
                utilizaremos a SMOTE em conjunto com o algoritmo de vizinhos-mais-próximos editados (edited nearest-neighbours — ENN, em inglês).
                o ENN é utilizado como método de “limpeza” do espaço resultante após a super-amostragem via SMOTE.
        Random Forest Classifier para fazer a previsão de transações fraudulentas.
        adicionar uma nova dimensão à nossa análise e conferir a Área Sob a Característica de Operação do Receptor (Area Under the Receiver-Operating Characteristic — AUROC, em inglês). 
        Intuitivamente, a AUROC indica o quão provável é que o seu modelo faça a distinção entre as duas classes.
        Em outras palavras, se você selecionar aleatoriamente uma observação de cada classe, qual a probabilidade do seu modelo ser capaz de “ordená-las” corretamente?
        SMOTE
            é importante analisar o impacto financeiro dessa mudança (diminuição da precisão e aumento do recall ):
                prejuízo financeiro decorrente de um aumento de falsos negativos, correspondendo a um decréscimo da precisão na detecção de fraudes.
                poderíamos perder clientes ao classificar suas transações erroneamente como fraudes, além de resultar em maiores custos operacionais decorrentes do cancelamento, emissão e envio de novos cartões para os clientes.
        RandomUnderSampler
            A sub-amostragem se mostrou uma péssima abordagem para esse problema.
            melhora recall e piora precisão
        SMOTE + ENN
            A SMOTE + ENN mostrou-se a melhor abordagem em nosso exemplo.
            reduz precisão e aumenta recall

    
    5) REF: https://github.com/juniorcl/transaction-fraud-detection
    https://www.overleaf.com/